Wikipediaの記事から位置情報をマイニング

Wikipediaの記事に含まれる位置情報をマイニングしてデータベース化しやすいCSVを生成します

使用方法:
http://dumps.wikimedia.org/jawiki/latest/
からjawiki-latest-pages-articles.xml.bz2をダウンロード解凍し、
 python src/coordinate.py XMLへのパス
のコマンドを実行するとWikipediaのダンプXMLから抽出したデータが標準出力に表示されます。データを書き出す場合はリダイレクトを使用してください。

最新のデータでなくても構わないならwiki_place.csvを使用してください

抽出可能形式:
src/regex_test.pyに正規表現のテストコードがあるのでそちらを参照してください

抽出優先順位:
 1. [ウィキ座標, coord] + display=title
 2. infobox内のdisplay=inline
 3. infobox内の日本語記述 | 緯度度 ... | 経度度 ...

出力形式: title|type|lat|lng
'|'区切りなのはtitleに','が含まれる記事が存在するのでエラーになるためです。

今後の発展:
 対応記事の拡張
 リダイレクト情報の活用（例えば現状では"東京国際展示場"は抽出できるが、"ビッグサイト"では抽出できない。リダイレクト情報を活用すれば"東京国際展示場"="ビッグサイト"の対応付けが可能となるはず）
